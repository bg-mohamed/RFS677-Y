{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import re,string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.<span style=\"color:red\"> EXTRACTION & COLLECTE DES TWEETS </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration \n",
    "#c = twint.Config()\n",
    "#c.Search =\"europeansuperleague OR superleague européene\"  #Topic de recherche\n",
    "#c.Lang = \"fr\" #langue des Tweets\n",
    "#c.Store_csv = True #Type de storage\n",
    "#c.Output = \"data/ESL.CSV\" #Adresse du storage\n",
    "#c.Pandas= True #conversion vers df\n",
    "#c.Since = \"2021-04-18\" #date de début\n",
    "#twint.run.Search(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ESL.csv\")\n",
    "df = df.loc[df['language'] == \"fr\"]\n",
    "df = df.loc[:, ['date', 'time','tweet', 'likes_count','retweets_count']]\n",
    "df = df.drop_duplicates(subset='tweet', keep=\"first\") #Suppression des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].describe(datetime_is_numeric=True).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <span style=\"color:blue\">  PREPROCESSING</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 <span style=\"color:black\">  ANALYSE DESCRIPTIVE</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion de la colonne tweet en liste\n",
    "twt_list=df['tweet'].tolist()\n",
    "print(f\"La liste est composée de {len(twt_list)} tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2390\n",
    "\n",
    "for twt in twt_list:\n",
    "    if i <= len(twt_list):\n",
    "        print(f\"Le tweet {i} est de longueur : {len(twt)}\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2398\n",
    "\n",
    "for twt in twt_list:\n",
    "    liste_mots = twt.split() # Permet de séparer les mots d'une chaines de caractère en fonction d'un séparateur \n",
    "                             # par défault l'espace.\n",
    "    if i <= len(twt_list):    \n",
    "        print(f\"Le tweet {i} contient {len(liste_mots)} mots\")\n",
    "        print(f\"La liste de mots du tweet {i} : {liste_mots} \\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Les 10 tweets ayant sucités le plus de réactions\n",
    "df.sort_values(['likes_count','retweets_count'],ascending = (False, False)).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 <span style=\"color:black\">  NETTOYAGE & TRANSFORMATION DE LA LISTE DES TWEETS</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = ['a', 'abord', 'absolument', 'afin', 'ah', 'ai', 'aie', 'ailleurs', 'ainsi', 'ait', 'allaient', 'allo', 'allons', \n",
    "             'allô', 'alors', 'anterieur', 'anterieure', 'anterieures', 'apres', 'après', 'as', 'assez', 'attendu', 'au', \n",
    "             'aucun', 'aucune', 'aujourd', \"aujourd'hui\", 'aupres', 'auquel', 'aura', 'auraient', 'aurait', 'auront', 'aussi', \n",
    "             'autre', 'autrefois', 'autrement', 'autres', 'autrui', 'aux', 'auxquelles', 'auxquels', 'avaient', 'avais', 'avait', \n",
    "             'avant', 'avec', 'avoir', 'avons', 'ayant', 'bah', 'bas', 'basee', 'bat', 'beau', 'beaucoup', 'bien', 'bigre', 'boum', \n",
    "             'bravo', 'brrr', \"c'\", 'car', 'ce', 'ceci', 'cela', 'celle', 'celle-ci', 'celle-là', 'celles', 'celles-ci', 'celles-là', \n",
    "             'celui', 'celui-ci', 'celui-là', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'ces', \n",
    "             'cet', 'cette', 'ceux', 'ceux-ci', 'ceux-là', 'chacun', 'chacune', 'chaque', 'cher', 'chers', 'chez', 'chiche', 'chut', \n",
    "             'chère', 'chères', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantième', 'cinquième', 'clac', 'clic', 'combien', \n",
    "             'comme', 'comment', 'comparable', 'comparables', 'compris', 'concernant', 'contre', 'couic', 'crac', 'c’', \"d'\", 'da', \n",
    "             'dans', 'de', 'debout', 'dedans', 'dehors', 'deja', 'delà', 'depuis', 'dernier', 'derniere', 'derriere', 'derrière', \n",
    "             'des', 'desormais', 'desquelles', 'desquels', 'dessous', 'dessus', 'deux', 'deuxième', 'deuxièmement', 'devant', 'devers', \n",
    "             'devra', 'different', 'differentes', 'differents', 'différent', 'différente', 'différentes', 'différents', 'dire', \n",
    "             'directe', 'directement', 'dit', 'dite', 'dits', 'divers', 'diverse', 'diverses', 'dix', 'dix-huit', 'dix-neuf', \n",
    "             'dix-sept', 'dixième', 'doit', 'doivent', 'donc', 'dont', 'douze', 'douzième', 'dring', 'du', 'duquel', 'durant', 'dès', \n",
    "             'désormais', 'd’', 'effet', 'egale', 'egalement', 'egales', 'eh', 'elle', 'elle-même', 'elles', 'elles-mêmes', 'en', \n",
    "             'encore', 'enfin', 'entre', 'envers', 'environ', 'es', 'est', 'et', 'etaient', 'etais', 'etait', 'etant', 'etc', 'etre', \n",
    "             'eu', 'euh', 'eux', 'eux-mêmes', 'exactement', 'excepté', 'extenso', 'exterieur', 'fais', 'faisaient', 'faisant', 'fait', \n",
    "             'façon', 'feront', 'fi', 'flac', 'floc', 'font', 'gens', 'ha', 'hein', 'hem', 'hep', 'hi', 'ho', 'holà', 'hop', 'hormis', \n",
    "             'hors', 'hou', 'houp', 'hue', 'hui', 'huit', 'huitième', 'hum', 'hurrah', 'hé', 'hélas', 'i', 'il', 'ils', 'importe', \n",
    "             \"j'\", 'je', 'jusqu', 'jusque', 'juste', 'j’', \"l'\", 'la', 'laisser', 'laquelle', 'las', 'le', 'lequel', 'les', \n",
    "             'lesquelles', 'lesquels', 'leur', 'leurs', 'longtemps', 'lors', 'lorsque', 'lui', 'lui-meme', 'lui-même', 'là', 'lès', 'l’', \n",
    "             \"m'\", 'ma', 'maint', 'maintenant', 'mais', 'malgre', 'malgré', 'maximale', 'me', 'meme', 'memes', 'merci', 'mes', 'mien', 'mienne', \n",
    "             'miennes', 'miens', 'mille', 'mince', 'minimale', 'moi', 'moi-meme', 'moi-même', 'moindres', 'moins', 'mon', \n",
    "             'moyennant', 'même', 'mêmes', 'm’', \"n'\", 'na', 'naturel', 'naturelle', 'naturelles', 'ne', 'neanmoins', 'necessaire', \n",
    "             'necessairement', 'neuf', 'neuvième', 'ni', 'nombreuses', 'nombreux', 'non', 'nos', 'notamment', 'notre', 'nous', 'nous-mêmes', \n",
    "             'nouveau', 'nul', 'néanmoins', 'nôtre', 'nôtres', 'n’', 'o', 'oh', 'ohé', 'ollé', 'olé', 'on', 'ont', 'onze', 'onzième', 'ore', \n",
    "             'ou', 'ouf', 'ouias', 'oust', 'ouste', 'outre', 'ouvert', 'ouverte', 'ouverts', 'où', 'paf', 'pan', 'par', 'parce', 'parfois', \n",
    "             'parle', 'parlent', 'parler', 'parmi', 'parseme', 'partant', 'particulier', 'particulière', 'particulièrement', 'pas', 'passé', \n",
    "             'pendant', 'pense', 'permet', 'personne', 'peu', 'peut', 'peuvent', 'peux', 'pff', 'pfft', 'pfut', 'pif', 'pire', 'plein', 'plouf', \n",
    "             'plus', 'plusieurs', 'plutôt', 'possessif', 'possessifs', 'possible', 'possibles', 'pouah', 'pour', 'pourquoi', 'pourrais', 'pourrait', \n",
    "             'pouvait', 'prealable', 'precisement', 'premier', 'première', 'premièrement', 'pres', 'probable', 'probante', 'procedant', 'proche', \n",
    "             'près', 'psitt', 'pu', 'puis', 'puisque', 'pur', 'pure', \"qu'\", 'quand', 'quant', 'quant-à-soi', 'quanta', 'quarante', 'quatorze', \n",
    "             'quatre', 'quatre-vingt', 'quatrième', 'quatrièmement', 'que', 'quel', 'quelconque', 'quelle', 'quelles', \"quelqu'un\", 'quelque', \n",
    "             'quelques', 'quels', 'qui', 'quiconque', 'quinze', 'quoi', 'quoique', 'qu’', 'rare', 'rarement', 'rares', 'relative', 'relativement', \n",
    "             'remarquable', 'rend', 'rendre', 'restant', 'reste', 'restent', 'restrictif', 'retour', 'revoici', 'revoilà', 'rien', \"s'\", 'sa', \n",
    "             'sacrebleu', 'sait', 'sans', 'sapristi', 'sauf', 'se', 'sein', 'seize', 'selon', 'semblable', 'semblaient', 'semble', 'semblent', \n",
    "             'sent', 'sept', 'septième', 'sera', 'seraient', 'serait', 'seront', 'ses', 'seul', 'seule', 'seulement', 'si', 'sien', 'sienne', \n",
    "             'siennes', 'siens', 'sinon', 'six', 'sixième', 'soi', 'soi-même', 'soit', 'soixante', 'son', 'sont', 'sous', 'souvent', 'specifique', \n",
    "             'specifiques', 'speculatif', 'stop', 'strictement', 'subtiles', 'suffisant', 'suffisante', 'suffit', 'suis', 'suit', 'suivant', \n",
    "             'suivante', 'suivantes', 'suivants', 'suivre', 'superpose', 'sur', 'surtout', 's’', \"t'\", 'ta', 'tac', 'tant', 'tardive', 'te', \n",
    "             'tel', 'telle', 'tellement', 'telles', 'tels', 'tenant', 'tend', 'tenir', 'tente', 'tes', 'tic', 'tien', 'tienne', 'tiennes', \n",
    "             'tiens', 'toc', 'toi', 'toi-même', 'ton', 'touchant', 'toujours', 'tous', 'tout', 'toute', 'toutefois', 'toutes', 'treize', 'trente', \n",
    "             'tres', 'trois', 'troisième', 'troisièmement', 'trop', 'très', 'tsoin', 'tsouin', 'tu', 'té', 't’', 'un', 'une', 'unes', \n",
    "             'uniformement', 'unique', 'uniques', 'uns', 'va', 'vais', 'vas', 'vers', 'via', 'vif', 'vifs', 'vingt', 'vivat', 'vive', 'vives', \n",
    "             'vlan', 'voici', 'voilà', 'vont', 'vos', 'votre', 'vous', 'vous-mêmes', 'vu', 'vé', 'vôtre', 'vôtres', 'zut', 'à', 'â', 'ça', 'ès', \n",
    "             'étaient', 'étais', 'était', 'étant', 'été', 'être', 'ô','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', \n",
    "             'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'qu', \"»\", \"«\",'quelqu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symboles & pictographes\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & cartographie\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # Drapeaux (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)  \n",
    "\n",
    "def emoji(string):\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cleaner(pandasSeries, stopWords):\n",
    "    \n",
    "    print(\"#### Nettoyage en cours ####\") \n",
    "    \n",
    "    # confirmation que chaque article est bien de type str\n",
    "    pandasSeries = pandasSeries.apply(lambda x : str(x))\n",
    "    \n",
    "    # Passage en minuscule\n",
    "    print(\"... Passage en minuscule\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x : x.lower())\n",
    "    \n",
    "    #Suppression de la ponctuation\n",
    "    print(\"... Suppression de la ponctuation\")\n",
    "    pandasSeries = pandasSeries.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "    \n",
    "    # Suppression des urls\n",
    "    print(\"... Suppression des urls\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x :re.sub(r\"http\\S+\", '', x)) \n",
    "    \n",
    "    # Suppression des pseudos\n",
    "    print(\"... Suppression des pseudos\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x :re.sub(r\"@\\S+\", '', x))\n",
    "    \n",
    "    # Suppression des hashtags\n",
    "    print(\"... Suppression des hashtags\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x :re.sub(r\"#\\S+\", '', x))\n",
    "   \n",
    "    # Suppression des emojis\n",
    "    print(\"... Suppression des emojis\") \n",
    "    pandasSeries = pandasSeries.apply(emoji)\n",
    "    \n",
    "    # Suppression des stop words\n",
    "    print(\"... Suppression des stop words\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x:' '.join([word for word in x.split() if word not in stopWords]))\n",
    "    \n",
    "    print(\"#### Nettoyage OK! ####\")\n",
    "\n",
    "    return pandasSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df['tweet_clean'] = df_cleaner(df['tweet'], stopWords)\n",
    "\n",
    "df[['tweet', 'tweet_clean']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <span style=\"color:GREEN\">  DATA VIZ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 <span style=\"color:black\">  Nombre de mots </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = (df['tweet_clean'].str.split(expand=True).stack().value_counts().rename_axis('mots').reset_index(name='Occurrences')).set_index('mots')\n",
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(20).plot(kind = \"bar\", figsize=(12, 8), color = \"Green\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 <span style=\"color:black\"> Bigrammes </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['tweet_clean'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrammes dans un document\n",
    "print(f\"Notre document n° 77: {corpus[77]} \\n\")\n",
    "\n",
    "\n",
    "print(f\"La liste des bigrammes présents dans le document n°77 : {list(nltk.bigrams(corpus[77]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 <span style=\"color:black\"> Nuages de mots </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 1200,height = 1000,scale = 2, background_color='black',collocations=True).generate(' '.join(df[\"tweet_clean\"]))\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <span style=\"color:Orange\">  Classification non supervisée </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Unsupervised.png](https://i.postimg.cc/TYBv1nJ3/Unsupervised.png)](https://postimg.cc/svPN0QLk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering**  : c'est une méthode de regroupement des objets en groupes de telle sorte que ceux qui présentent le plus de similitudes restent dans un groupe et présentent moins ou pas de similitudes avec les objets d'un autre groupe. L'analyse de cluster trouve les points communs entre les objets de données et les catégorise selon la présence et l'absence de ces points communs.\n",
    "\n",
    "**Association**: Une règle d'association est une méthode d'apprentissage non supervisée qui est utilisée pour trouver les relations entre les variables dans une base de données. Il détermine l'ensemble des éléments qui se produisent ensemble dans l'ensemble de données. La règle d'association peut rendre une stratégie marketing plus efficace. Comme les personnes qui achètent un article X (supposons un pain) ont également tendance à acheter un article Y (beurre/confiture). Un exemple typique de règle d'association est l'analyse du panier de marché."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parmi les algorithmes les plus utilisés dans l'apprentissage non supervisé :\n",
    "\n",
    "- K-means clustering  \n",
    "- KNN (k-nearest neighbors)\n",
    "- Hierarchal clustering\n",
    "- Topic Modelling (LDA / NMF)\n",
    "- Anomaly detection\n",
    "- Neural Networks\n",
    "- Principle Component Analysis\n",
    "- Independent Component Analysis\n",
    "- Apriori algorithm\n",
    "- Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 <span style=\"color:black\">  K-means Clustering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.1.1 <span style=\"color:black\">  TF-IDF</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le ratio TF-IDF (Term-Frequency - Inverse Document Frequency) permet de pondérer la fréquence des tokens (mots dans notre corpus) dans un document par son importance relative dans les autres documents.\n",
    "\n",
    "Ce score tient donc compte de la composition de notre corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorisation des trigram et avec un minimum d'apparitions de 2 fois\n",
    "vectorizer = TfidfVectorizer(ngram_range=range(1,3), min_df=2,stop_words=stopWords)\n",
    "X = vectorizer.fit_transform(df['tweet_clean'])\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mots = ' '.join(df['tweet_clean']) # Concaténation des documents renvoi une chaine de caractères\n",
    "\n",
    "liste_mots = re.findall(r\"\\w+\", mots) # Création de la liste contenant tous les mots de notre corpus\n",
    "print(f\"Ci-dessous, la liste de mots présents dans le corpus : \\n{liste_mots}\\n\")\n",
    "\n",
    "vocab = set(liste_mots)\n",
    "print(f\"Le vocabulaire est de taille {len(vocab)}\")\n",
    "print(f\"Ci-dessous, le vocabulaire (mots uniques) : \\n{vocab}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du dictionnaire\n",
    "freq = defaultdict(int)\n",
    "\n",
    "# Compte l'ocurrence de chaque mot du corpus\n",
    "for mot in liste_mots:\n",
    "    freq[mot] += 1\n",
    "    \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.1.2 <span style=\"color:black\">  Choix du nombre de clusters `true_k` avec la méthode \"elbow\"</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le choix du true_k se fera graphiquement en prenant l'abcisse du coude correspondant à une faible pente du graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,15), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Nombre de Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.1.3 <span style=\"color:black\">  Application de l'algorithme K-means</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "true_k = 9\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=1000, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top termes par cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.1.4 <span style=\"color:black\">  Test de prédiction du cluster</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "X = vectorizer.transform([\"est ce que Liverpool a rejoint la super league européene?\"])\n",
    "predicted = model.predict(X)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 <span style=\"color:black\">  Topic Modelling (LatentDirichletAllocation)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.2.1 <span style=\"color:black\">  Vectorisation & Transformation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtre appliqué sur les mots qui apparaissent dans moins de 10% des tweets \n",
    "vectorizer = CountVectorizer(min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = vectorizer.fit_transform(df['tweet_clean']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.2.2 <span style=\"color:black\">  Modélisation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choix de nombre de topics arbitraire\n",
    "number_of_topics = 6\n",
    "\n",
    "model2 = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
    "model2.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_top_words = 10\n",
    "display_topics(model2, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans trop de surprises, les termes ayant le plus de poids (occurences) dans les tweets sont :\n",
    "- super league européenne\n",
    "- UEFA / FIFA\n",
    "- 12 clubs fondateurs de l'ESL\n",
    "- Florentino Perez le président du Réal Madrid par celui l'annonce est venue\n",
    "- PSG / Real / Manchester U/ Manchester City/ Arsenal / Milan : qui sont les clubs protagonistes ou antagonistes du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 <span style=\"color:black\">  Topic Modelling (Non-Negative Matrix Factorization)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = TfidfVectorizer(min_df=25, stop_words=stopWords)\n",
    "# Fit and transform\n",
    "F = D.fit_transform(df.tweet_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = NMF(n_components=6, random_state=5)\n",
    "model3.fit(F)\n",
    "nmf_features = model3.transform(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_df = pd.DataFrame(model3.components_, columns=D.get_feature_names())\n",
    "components_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for topic in range(components_df.shape[0]):\n",
    "    tmp = components_df.iloc[topic]\n",
    "    print(f'les mots les plus récurrents du {topic+1} :')\n",
    "    print(tmp.nlargest(10))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
